# -*- coding: utf-8 -*-
"""mnist digits using keras using CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ItHMGZTi3wHdk5Jq_TgXRbKkjaib0Jt5

## Now Start to Develop a Convolutional Neural Network to classify MNIST Handwritten Digits images.
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from matplotlib import pyplot as plt
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras import layers

"""MNIST Dataset
The MNIST dataset is an acronym that stands for the Modified National Institute of Standards and Technology dataset. It is a dataset of 60,000 small square 28Ã—28 pixel grayscale images of handwritten single digits between 0 and 9.
"""

(x_train, y_train), (x_test, y_test) = mnist.load_data()
#loaded datasets summery
print("Train:","x=",x_train.shape,"y=",y_train.shape)
print("Test:","x=",x_test.shape,"y=",y_test.shape)

x_train = x_train.astype('float')/255.
x_test = x_test.astype('float')/255.

"""## To verify that the dataset looks correct, let's plot the first 10 images from the training set and display the class name below each image."""

plt.figure(figsize=(20,2))
for i in range(10):
        plt.subplot(1,10,i+1)
        plt.imshow(x_train[i], cmap='binary')
        plt.xticks([])
        plt.yticks([])
        # The labels happen to be arrays, 
        # which is why you need the extra index
        plt.xlabel(y_train[i])

# reshape dataset to have a single channel
x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))
x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))

model = Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28,1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.summary()

model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

model.summary()

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=10, 
                    validation_data=(x_test, y_test))

#Saving trained model

model.save('my_model_digits.h5')
print("Saved model to disk")

#Saving the model

from tensorflow.keras.models import load_model

model=load_model('my_model_digits.h5')

# make a prediction for a new image.
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.models import load_model
 
# load and prepare the image
def load_image(filename):
	# load the image
	img = load_img(filename, grayscale=True, target_size=(28, 28))
	# convert to array
	img = img_to_array(img)
	# reshape into a single sample with 1 channel
	img = img.reshape(1, 28, 28, 1)
	# prepare pixel data
	img = img.astype('float32')
	img = img / 255.0
	return img
 
# load an image and predict the class
def predict_sample():
	# load the image
	img = load_image('sample2.jpg')
	# load model
	model = load_model('my_model_digits.h5')
	# predict the class
	digit = model.predict_classes(img)
	print(digit[0])
 
# entry point, run the example
predict_sample()



"""## Evaluate the model"""

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)

print(test_acc)

